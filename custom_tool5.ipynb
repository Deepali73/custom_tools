{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23ab53e-37f1-49f6-aab2-addc10a4fa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20704\\364379474.py:43: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Package Picker Agent is Ready!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "What do you want to do with Python?\n",
      ">  web scraping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: web scraping\n",
      "Thought: I need to find suitable Python packages for web scraping.  I can use the `suggest_python_packages` tool for this.\n",
      "Action: suggest_python_packages\n",
      "Action Input: web scraping\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mAmbiguous task. Try one of these: Spider web, World Wide Web, WEB\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mQuestion: web scraping\n",
      "Thought: I need to find suitable Python packages for web scraping.  I can use the `suggest_python_packages` tool for this.  The previous response indicated ambiguity, so I'll try specifying \"World Wide Web\" as the target.\n",
      "Action: suggest_python_packages\n",
      "Action Input: World Wide Web scraping\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mWikipedia Summary:\n",
      "The English modal auxiliary verbs are a subset of the English auxiliary verbs used mostly to express modality, properties such as possibility and obligation. They can most easily be distinguished from other verbs by their defectiveness (they do not have participles or plain forms) and by their lack of the ending â€‘(e)s for the third-person singular.\n",
      "\n",
      "Gemini Suggestions:\n",
      "content='For web scraping in Python, several excellent libraries offer different strengths.  There\\'s no single \"best\" library, as the optimal choice depends on the complexity of the target website and your specific needs.  However, here\\'s a breakdown of top contenders and their use cases:\\n\\n**1. `requests`:**\\n\\n* **Purpose:**  Fetching web pages. This is the foundational library for almost all web scraping projects.  It handles HTTP requests, allowing you to download the HTML content of a URL.\\n* **Why it\\'s useful:**  Simple, efficient, and widely used.  It\\'s the backbone for more advanced libraries.  Handles headers, cookies, and various HTTP methods seamlessly.\\n* **Alternatives:**  `urllib3` (requests uses this under the hood), `httpx` (a more modern alternative with async support).  However, `requests` remains the most popular and user-friendly choice for most scenarios.\\n\\n**2. `Beautiful Soup 4` (bs4):**\\n\\n* **Purpose:** Parsing HTML and XML.  Once you\\'ve fetched a webpage with `requests`, you use Beautiful Soup to navigate and extract the data you need.\\n* **Why it\\'s useful:**  Provides a simple and intuitive API for traversing the DOM (Document Object Model) of an HTML page.  Handles messy or malformed HTML gracefully.  Supports various parsers (like lxml and html5lib) for different performance needs.\\n* **Alternatives:**  `lxml` (faster but steeper learning curve), `html5lib` (robust handling of invalid HTML).  Beautiful Soup often acts as a wrapper around these, providing a consistent interface.  Choose `lxml` if performance is critical and you\\'re comfortable with its XML-centric approach.\\n\\n**3. `Scrapy`:**\\n\\n* **Purpose:**  A full-fledged web scraping framework.  It\\'s much more than just a library; it\\'s a complete system for building web scrapers.\\n* **Why it\\'s useful:**  Handles asynchronous requests, provides built-in mechanisms for managing requests, handling responses, and storing data.  Offers features like middleware for handling proxies, user agents, and robots.txt.  Excellent for large-scale scraping projects.  Highly scalable and extensible.\\n* **Alternatives:**  `Selenium` (for dynamic websites requiring JavaScript rendering), `Playwright` (similar to Selenium but often faster and more reliable).  Scrapy is preferred when you need a robust, structured approach to large-scale scraping.\\n\\n**4. `Selenium` / `Playwright`:**\\n\\n* **Purpose:**  Automating web browsers.  Essential when dealing with websites that heavily rely on JavaScript to render content.\\n* **Why they\\'re useful:**  These libraries control a real browser (like Chrome or Firefox), allowing you to interact with the page as a user would, including executing JavaScript.  This is crucial for scraping dynamic content that isn\\'t available in the initial HTML source.\\n* **Alternatives:**  `Puppeteer` (Node.js based, but can be used with Python bindings).  Choose Selenium for its maturity and wide community support, or Playwright for its speed and cross-browser compatibility.\\n\\n**Example combining `requests` and `Beautiful Soup`:**\\n\\n```python\\nimport requests\\nfrom bs4 import BeautifulSoup\\n\\nurl = \"https://www.example.com\"\\nresponse = requests.get(url)\\nresponse.raise_for_status() # Raise an exception for bad status codes\\n\\nsoup = BeautifulSoup(response.content, \"html.parser\")\\ntitle = soup.title.string\\nprint(f\"Page title: {title}\")\\n\\n# Extract other data as needed using soup.find(), soup.find_all(), etc.\\n```\\n\\nIn summary:  Start with `requests` and `Beautiful Soup` for simpler tasks.  If you need asynchronous requests, robust error handling, or large-scale scraping, consider `Scrapy`.  For dynamic websites, `Selenium` or `Playwright` are necessary.  Remember to always respect the website\\'s `robots.txt` and terms of service.  Overly aggressive scraping can lead to your IP being blocked.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--36aa2285-cac6-4eaf-b98c-1dceb1dfc1d3-0' usage_metadata={'input_tokens': 32, 'output_tokens': 880, 'total_tokens': 912, 'input_token_details': {'cache_read': 0}}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know the final answer. The observation provides a comprehensive list of Python packages suitable for web scraping, categorized by their strengths and use cases.\n",
      "Final Answer: The best Python packages for web scraping depend on the complexity of the task and the target website.  For simpler tasks, `requests` (for fetching web pages) and `Beautiful Soup 4` (for parsing HTML) are a good starting point. For larger scale projects or those requiring asynchronous requests, `Scrapy` is a powerful framework.  If the website uses JavaScript to render content, `Selenium` or `Playwright` are necessary to automate a browser.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      " Output:\n",
      " {'input': 'web scraping', 'output': 'The best Python packages for web scraping depend on the complexity of the task and the target website.  For simpler tasks, `requests` (for fetching web pages) and `Beautiful Soup 4` (for parsing HTML) are a good starting point. For larger scale projects or those requiring asynchronous requests, `Scrapy` is a powerful framework.  If the website uses JavaScript to render content, `Selenium` or `Playwright` are necessary to automate a browser.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wikipedia\n",
    "from langchain.agents import tool, initialize_agent, AgentType\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Set Gemini Key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCVp_\"\n",
    "\n",
    "# Load Gemini model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
    "\n",
    "# Tool for recommending Python packages\n",
    "@tool\n",
    "def suggest_python_packages(task: str) -> str:\n",
    "    \"\"\"\n",
    "    Suggests Python packages for a given task or use case.\n",
    "    Example: 'data visualization', 'web scraping', 'automation'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Search Wikipedia\n",
    "        keyword = task.split()[0]\n",
    "        wiki_summary = wikipedia.summary(keyword, sentences=2)\n",
    "\n",
    "        # Prompt Gemini\n",
    "        prompt = (\n",
    "            f\"You are a Python expert. Suggest the best Python libraries for this task: '{task}'. \"\n",
    "            f\"Explain why each is useful. Mention alternatives if needed.\"\n",
    "        )\n",
    "        suggestions = llm.invoke(prompt)\n",
    "\n",
    "        return f\"Wikipedia Summary:\\n{wiki_summary}\\n\\nGemini Suggestions:\\n{suggestions}\"\n",
    "    \n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return f\"Ambiguous task. Try one of these: {', '.join(e.options[:3])}\"\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return \"Wikipedia: No page found. Try simpler task keywords.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Initialize LangChain Agent\n",
    "tools = [suggest_python_packages]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run via CLI\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Python Package Picker Agent is Ready!\")\n",
    "    user_input = input(\"What do you want to do with Python?\\n> \")\n",
    "    result = agent.invoke(user_input)\n",
    "    print(\"\\n Output:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d41416-8114-4766-a1dc-fd9ff21e7d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Suggestions : web scraping,\n",
    "               text generation,\n",
    "               image classification,\n",
    "               automated testing,\n",
    "               PDF report creation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
